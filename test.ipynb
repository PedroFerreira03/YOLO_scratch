{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559edfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def IOU(box1, box2):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        box1 (batch_size, N, 1, 5): Golden label box\n",
    "        box2 (batch_size, N, B, 5): All boxes of that cell \n",
    "    \"\"\"\n",
    "    if isinstance(box1, list):\n",
    "        box1 = torch.tensor(box1, dtype=float)\n",
    "\n",
    "    if isinstance(box2, list):\n",
    "        box2 = torch.tensor(box2, dtype=float)\n",
    "\n",
    "    _, x_mid1, y_mid1, width1, height1 = box1.unbind(dim=-1) # Removes one dimension and enumerates the resulting tensor. dim = -1 will correspond to the columns\n",
    "    _, x_mid2, y_mid2, width2, height2 = box2.unbind(dim=-1)\n",
    "\n",
    "    x11, y11, x21, y21 = get_x1y1x2y2(x_mid1, y_mid1, width1, height1)\n",
    "    x12, y12, x22, y22 = get_x1y1x2y2(x_mid2, y_mid2, width2, height2)\n",
    "\n",
    "    dx = torch.minimum(x21, x22) - torch.maximum(x11, x12)\n",
    "    dy = torch.minimum(y21, y22) - torch.maximum(y11, y12)\n",
    "    dx = (dx > 0) * dx # Mask if dx < 0 -> No intersection because of x-axis\n",
    "    dy = (dy > 0) * dy # Mask if dy < 0 -> No intersection because of y-axis\n",
    "    area_intersection = dx * dy\n",
    "    \n",
    "    area_union = width1 * height1 + width2 * height2 - area_intersection\n",
    "    clipped_area_union = torch.clamp(area_union, min=1e-6)\n",
    "\n",
    "    return area_intersection/clipped_area_union\n",
    "\n",
    "def get_x1y1x2y2(x_mid, y_mid, width, height):\n",
    "    half_height = height / 2\n",
    "    half_width = width / 2\n",
    "    \n",
    "    return x_mid - half_width, y_mid - half_height, x_mid + half_width, y_mid + half_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3fb02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class YOLO(torch.nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=200, slope=0.1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S (int): Dimensions for the final grid SxS\n",
    "            B (int): Number of boxes per position on the grid\n",
    "            C (int): Number of classes \n",
    "            slope (float): Negative slope in LeakyRELU\n",
    "            dropout (float): Dropout Probability\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Initializing attributes\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # Block 1\n",
    "        conv1 = self.conv_block(3, [7], [64], [2], [2], [2], slope)\n",
    "\n",
    "        # Block 2\n",
    "        conv2 = self.conv_block(64, [3], [192], [1], [2], [2], slope)\n",
    "\n",
    "        # Block 3\n",
    "        conv3 = self.conv_block(192, [1, 3, 1, 3], [128, 256, 256, 512], [1] * 4, [2], [2], slope)\n",
    "    \n",
    "        # Block 4\n",
    "        conv4 = self.conv_block(512, [1, 3] * 5, [256, 512] * 4 + [512, 1024], [1] * 10, [2], [2], slope)\n",
    "\n",
    "        # Block 5\n",
    "        #conv5 = self.conv_block(1024, [1, 3] * 2 + [3, 3], [512, 1024] * 2 + [1024] * 2, [1] * 5 + [2], [], [], slope)\n",
    "\n",
    "        # Block 6\n",
    "        #conv6 = self.conv_block(1024, [3, 3], [1024, 1024], [1, 1], [], [], slope)\n",
    "\n",
    "        # Flatten layer\n",
    "        flatten = nn.Flatten()\n",
    "\n",
    "        # First Dense Layer - Assumes images are 448x448x3, since it is faithful to the original YOLO\n",
    "        linear1 = nn.Linear(7*7*1024, 4096)\n",
    "\n",
    "        # Activation function after Linear Layer\n",
    "        act = nn.LeakyReLU(slope, inplace=True)\n",
    "\n",
    "        # Dropout\n",
    "        drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Last Dense Layer\n",
    "        linear2 = nn.Linear(4096, S*S*(B*5 + C))\n",
    "\n",
    "        # Create the layers object\n",
    "        self.layers = nn.Sequential(*conv1,\n",
    "                                    *conv2,\n",
    "                                    *conv3,\n",
    "                                    *conv4,\n",
    "                                    flatten,\n",
    "                                    linear1,\n",
    "                                    act,\n",
    "                                    drop,\n",
    "                                    linear2\n",
    "                                    )\n",
    "        \n",
    "        \n",
    "    def conv_block(self, start_channels, size_conv, out_channels, stride_conv, size_pool, stride_pool, slope):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            start_channels (int): Number of channels of first input.\n",
    "            out_channels (List[int]): Number of kernels for each convlutional layer\n",
    "            size_conv (List[int]): Filter sizes for each convolutional layer.\n",
    "            stride_conv (List[int]): Stride values for each convolutional layer.\n",
    "            size_pool (List[int]): Size for the single pooling layer (if exists)\n",
    "            stride_pool (List[int]): Stride for the single pooling layer (if exists)\n",
    "            slope (float): Negative slope in LeakyRELU\n",
    "        \n",
    "        Output:\n",
    "            layers (List[nn.Module]): List with all the layers of the block\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        in_channels = [start_channels] + out_channels[:-1]\n",
    "        for inp, out, size, stride in zip(in_channels, out_channels, size_conv, stride_conv):\n",
    "            layers.append(nn.Conv2d(inp, out, size, stride, size//2))\n",
    "            layers.append(nn.LeakyReLU(slope))\n",
    "\n",
    "        for size, stride in zip(size_pool, stride_pool):\n",
    "            layers.append(nn.MaxPool2d(size, stride))\n",
    "        \n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return torch.reshape(x, (batch_size, self.S, self.S, self.B*5 + self.C))\n",
    "\n",
    "    def predict(self, output, IOU_threshold=0.8, conf_threshold=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output (batch_size, S, S, 5*B + C)\n",
    "\n",
    "        Output:\n",
    "            output_boxes (batch_size, S*S*B, 6)\n",
    "        \"\"\"\n",
    "        # Get device\n",
    "        device = output.device\n",
    "\n",
    "        # First Step - Get the relevant information from the boxes\n",
    "        num_grid = self.S * self.S\n",
    "        num_boxes = num_grid * self.B\n",
    "\n",
    "        batch_size = output.shape[0]\n",
    "\n",
    "        boxes = output[:, :, :, :5*self.B].reshape((batch_size, num_boxes, 5)) # (batch_size, S*S*B, 5)\n",
    "        logits = output[:, :, :, 5*self.B:].reshape((batch_size, num_grid, self.C)) # (batch_size, S*S, 200)\n",
    "        classes = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        conf, x_mid, y_mid, width, height = boxes.unbind(dim=-1) # (batch_size, S*S*B)\n",
    "        x_mid, y_mid = self.grid2img(x_mid, y_mid) # Convert relative to grid to relative to image\n",
    "\n",
    "        class_prob, class_idx = torch.max(classes, dim=-1) # (batch_size, S*S)\n",
    "        \n",
    "        class_prob_repeated = class_prob.unsqueeze(-1).expand(batch_size, self.S * self.S, self.B)\\\n",
    "            .reshape((batch_size, num_boxes)) # (batch_size, S*S, B) -> (batch_size, S*S*B)\n",
    "        \n",
    "        class_idx_repeated = class_idx.unsqueeze(-1).expand(batch_size, self.S * self.S, self.B)\\\n",
    "            .reshape((batch_size, num_boxes)) # (batch_size, S*S, B) -> (batch_size, S*S*B)\n",
    "        \n",
    "        new_conf = torch.mul(conf, class_prob_repeated)  # (batch_size, S*S*B)\n",
    "\n",
    "        new_conf, x_mid, y_mid, width, height, class_idx_repeated = map(lambda z: z.unsqueeze(-1), \\\n",
    "                                                                        [new_conf, x_mid, y_mid, width, height, class_idx_repeated])\n",
    "        \n",
    "        x_mid, y_mid, width, height = map(lambda x: torch.clamp(x, min=0, max=1), [x_mid, y_mid, width, height])\n",
    "\n",
    "        output_boxes = torch.cat((new_conf, x_mid, y_mid, width, height, class_idx_repeated), dim=-1) # (batch_size, S*S*B, 6)\n",
    "\n",
    "\n",
    "        # Second step - Sort by conf\n",
    "        sorted_conf, sorted_idx = torch.sort(new_conf.squeeze(-1), dim=-1, descending=True) # (batch_size, num_boxes)\n",
    "        batch_idx = torch.arange(batch_size).unsqueeze(1).expand(batch_size, num_boxes)\n",
    "        sorted_boxes = output_boxes[batch_idx, sorted_idx, :]\n",
    "        \n",
    "\n",
    "        # Third Step - Non Maximum Supression (NMS)\n",
    "        all_mask = torch.ones((batch_size, num_boxes)).to(device)\n",
    "        for i in range(num_boxes-1):\n",
    "            curr_boxes = sorted_boxes[:, i, :-1] # (batch_size, 5)\n",
    "            other_boxes = sorted_boxes[:, i+1:, :-1] # (batch_size, num_boxes-i, 5)\n",
    "            IOU_results = IOU(curr_boxes.unsqueeze(1), other_boxes)\n",
    "            mask = IOU_results <= IOU_threshold\n",
    "            is_masked = all_mask[:, i].unsqueeze(-1)\n",
    "            all_mask[:, i+1:] = (1 - is_masked + is_masked * mask) * all_mask[:, i+1:] \n",
    "\n",
    "\n",
    "        # Fourth Step - Threshold for confidence and get Final Boxes\n",
    "        result = [[] for _ in range(batch_size)]\n",
    "        for batch in range(batch_size):\n",
    "            for box in range(num_boxes):\n",
    "               if sorted_conf[batch, box] >= conf_threshold:\n",
    "                   if all_mask[batch, box]:\n",
    "                        result[batch].append(tuple(sorted_boxes[batch, box, :].cpu().detach().tolist()))\n",
    "               else:\n",
    "                   break\n",
    "\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def grid2img(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (batch_size, S*S*B)\n",
    "            y (batch_size, S*S*B)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0] \n",
    "        device = x.device\n",
    "        new_dim = self.S * self.S * self.B\n",
    "        dx = torch.arange(self.S).unsqueeze(0).unsqueeze(0).unsqueeze(-1).expand(batch_size, self.S, self.S, self.B).reshape(batch_size, new_dim).to(device)\n",
    "        dy = torch.arange(self.S).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).expand(batch_size, self.S, self.S, self.B).reshape(batch_size, new_dim).to(device)\n",
    "        inv_S = 1/self.S\n",
    "\n",
    "        return (x + dx) * inv_S, (y + dy) * inv_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Cuda\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Get the weights\n",
    "weights_path = \"best_model.pth\"\n",
    "weights = torch.load(weights_path)\n",
    "\n",
    "# Load model\n",
    "model = YOLO().to(device)\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "# Transformation\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224\n",
    "    )),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def get_images_from_folder(folder_path, extensions=(\".jpeg\")):\n",
    "    return [\n",
    "        os.path.join(folder_path, f)\n",
    "        for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(extensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "def predict_and_show_inline(image_path):\n",
    "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "    np_img = np.array(pil_img)  \n",
    "    image_tensor = transform(pil_img).to(device).unsqueeze(0)\n",
    "\n",
    "    output = model(image_tensor)\n",
    "    predictions = model.predict(output)[0] \n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for pred in predictions:\n",
    "        conf, xm, ym, w, h, cls_id = pred\n",
    "\n",
    "        xmin = int(xm - w / 2)\n",
    "        ymin = int(ym - h / 2)\n",
    "        xmax = int(xm + w / 2)\n",
    "        ymax = int(ym + h / 2)\n",
    "\n",
    "        label = f\"{int(cls_id)} {conf:.2f}\"\n",
    "\n",
    "        # Draw bounding box & label\n",
    "        cv2.rectangle(np_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        cv2.putText(np_img, label, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0, 255, 0), 2)\n",
    "\n",
    "    if count > 0:\n",
    "        # Show in notebook\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(np_img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "folder_path = \"tiny-imagenet-200/test/images\"\n",
    "images = get_images_from_folder(folder_path)\n",
    "\n",
    "for img_path in images:\n",
    "    predict_and_show_inline(img_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
